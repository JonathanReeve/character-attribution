{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, lenMin=2000, lenMax=10000, chunks=False): \n",
    "        self.lenMin = lenMin\n",
    "        self.lenMax = lenMax\n",
    "        self.chunks=chunks\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        print('heyo! fitting')\n",
    "        return self\n",
    "\n",
    "    def transform(self, filename): \n",
    "        print('heyo! transforming')\n",
    "        lenMin, lenMax = self.lenMin, self.lenMax\n",
    "        self.tree = etree.parse(filename)\n",
    "        self.allSaidElems = self.tree.findall('.//said[@who]')\n",
    "        # Only get those in our length range\n",
    "        self.saidElems = [elem for elem in self.allSaidElems if len(elem.text)>lenMin and len(elem.text)<lenMax]\n",
    "        self.allChars = [elem.attrib['who'] for elem in self.saidElems]\n",
    "        self.chars = list(set(self.allChars))\n",
    "        self.labeledText = [(elem.attrib['who'], elem.text) for elem in self.saidElems]\n",
    "        self.labeledText = [(item[0], self.clean(item[1])) for item in self.labeledText]\n",
    "        self.labels = [item[0] for item in self.labeledText]\n",
    "        charDict = {'Bernard': 0, 'Louis': 1, 'Neville': 2,\n",
    "                'Rhoda': 3, 'Jinny': 4, 'Susan': 5}\n",
    "        self.numericLabels = [charDict[label] for label in self.labels]\n",
    "        self.allText = [item[1] for item in self.labeledText]\n",
    "        self.charDict = self.makeCharDict()\n",
    "        self.charChunks, self.charChunksLabels = self.makeCharChunks()\n",
    "        if self.chunks: \n",
    "            self.allText = self.charChunks\n",
    "            self.labels = self.charChunksLabels\n",
    "        self.lengths = [len(item) for item in self.allText]\n",
    "        return self.allText\n",
    "        \n",
    "    def makeCharDict(self): \n",
    "        \"\"\" Make a dictionary of each character's total speech. \"\"\"\n",
    "        # Initialize empty dictionary. \n",
    "        charDict = {char: \"\" for char in self.chars}\n",
    "        for elem in self.allSaidElems: \n",
    "            charDict[elem.attrib['who']]+=elem.text\n",
    "        return charDict\n",
    "            \n",
    "    def makeCharChunks(self, n=2): \n",
    "        \"\"\" Make a list of chunks of character speech. \"\"\"\n",
    "        charChunks = []\n",
    "        charChunksLabels = []\n",
    "        for char, text in self.charDict.items(): \n",
    "            #chunks = self.chunkText(text, n)\n",
    "            chunks = self.sliceText(text)\n",
    "            for i, chunk in enumerate(chunks): \n",
    "                charChunks.append(chunk)\n",
    "                charChunksLabels.append(char + '-%s' % i)\n",
    "        return charChunks, charChunksLabels\n",
    "        \n",
    "    def chunkText(self, text, n=2): \n",
    "        \"\"\" Breaks one text into n texts.\"\"\"\n",
    "        newLen = math.floor(len(text) / n)\n",
    "        parts = [text[i:i+newLen] for i in range(0, len(text), newLen)]\n",
    "        if len(parts) > n: \n",
    "            parts[-2]+=parts[-1]\n",
    "            parts = parts[:n]\n",
    "        return parts\n",
    "    \n",
    "    def sliceText(self, text, size=8000):\n",
    "        parts = []\n",
    "        while len(text) > size: \n",
    "            part = text[:size]\n",
    "            text = text[size:]\n",
    "            parts.append(part)\n",
    "        return parts\n",
    "\n",
    "    def clean(self, utterance): \n",
    "        \"\"\" \n",
    "        Cleans utterances. \n",
    "        \"\"\"\n",
    "        # Remove \"said Bernard,\" etc. \n",
    "        charRegex = \"said (%s),\" % '|'.join(self.chars)\n",
    "        out = re.sub(charRegex, '', utterance)\n",
    "       \n",
    "        # Remove quotation marks. \n",
    "        out = re.sub('[“”\"]', '', out)\n",
    "        \n",
    "        # Remove line breaks. \n",
    "        out = re.sub('\\n', ' ', out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code adapted from http://stackoverflow.com/a/28384887/584121\n",
    "class DenseTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self): \n",
    "        return\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = { 'tfidf__use_idf': (True, False), \n",
    "               'tfidf__min_df': (0.0, 0.3, 0.4),\n",
    "               'tfidf__max_df': (0.5, 0.7, 1.0),\n",
    "               'tfidf__max_features': (100, 200, 500, 1000, 2000), \n",
    "               'pca__n_components': (2, 5, 10, 20, 25, 30, 50),\n",
    "               'bgm__n_components': (2, 4, 6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyo! fitting\n",
      "heyo! transforming\n"
     ]
    }
   ],
   "source": [
    "text = Text().fit()\n",
    "docs = text.transform('waves-tei.xml')\n",
    "labels = text.numericLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([  \n",
    "#                          ('text', Text()), \n",
    "                         ('tfidf', TfidfVectorizer()),\n",
    "                         ('todense', DenseTransformer()),\n",
    "                         ('pca', PCA(n_components=10)),\n",
    "                         ('bgm', BayesianGaussianMixture(n_components=6)),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'todense', 'pca', 'bgm']\n",
      "parameters:\n",
      "{'bgm__n_components': (2, 4, 6),\n",
      " 'pca__n_components': (2, 5, 10, 20, 25, 30, 50),\n",
      " 'tfidf__max_df': (0.5, 0.7, 1.0),\n",
      " 'tfidf__max_features': (100, 200, 500, 1000, 2000),\n",
      " 'tfidf__min_df': (0.0, 0.3, 0.4),\n",
      " 'tfidf__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 1890 candidates, totalling 5670 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n",
      "/usr/lib/python3.5/site-packages/sklearn/mixture/base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   14.3s\n",
      "/usr/lib/python3.5/site-packages/sklearn/mixture/base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 89.1min\n",
      "/usr/lib/python3.5/site-packages/sklearn/mixture/base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 90.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 90.6min\n",
      "/usr/lib/python3.5/site-packages/sklearn/mixture/base.py:237: ConvergenceWarning: Initialization 1 did not converged. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  % (init + 1), ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 91.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 92.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5586.093s\n",
      "\n",
      "Best score: 29.903\n",
      "Best parameters set:\n",
      "\tbgm__n_components: 2\n",
      "\tpca__n_components: 25\n",
      "\ttfidf__max_df: 1.0\n",
      "\ttfidf__max_features: 500\n",
      "\ttfidf__min_df: 0.0\n",
      "\ttfidf__use_idf: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 5670 out of 5670 | elapsed: 93.1min finished\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(docs, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
