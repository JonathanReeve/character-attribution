{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "# import edward as ed\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Text(): \n",
    "    def __init__(self, lenRange=(2000, 10000), chunks=False): \n",
    "        self.lenRange=lenRange\n",
    "        self.chunks=chunks\n",
    "        \n",
    "    def fit(self, *_):\n",
    "        print('heyo! fitting')\n",
    "        return self\n",
    "\n",
    "    def transform(self, filename): \n",
    "        print('heyo! transforming')\n",
    "        lenMin, lenMax = self.lenRange\n",
    "        self.tree = etree.parse(filename)\n",
    "        self.allSaidElems = self.tree.findall('.//said[@who]')\n",
    "        # Only get those in our length range\n",
    "        self.saidElems = [elem for elem in self.allSaidElems if len(elem.text)>lenMin and len(elem.text)<lenMax]\n",
    "        self.allChars = [elem.attrib['who'] for elem in self.saidElems]\n",
    "        self.chars = list(set(self.allChars))\n",
    "        self.labeledText = [(elem.attrib['who'], elem.text) for elem in self.saidElems]\n",
    "        self.labeledText = [(item[0], self.clean(item[1])) for item in self.labeledText]\n",
    "        self.labels = [item[0] for item in self.labeledText]\n",
    "        charDict = {'Bernard': 0, 'Louis': 1, 'Neville': 2,\n",
    "                'Rhoda': 3, 'Jinny': 4, 'Susan': 5}\n",
    "        self.numericLabels = [charDict[label] for label in self.labels]\n",
    "        self.allText = [item[1] for item in self.labeledText]\n",
    "        self.charDict = self.makeCharDict()\n",
    "        self.charChunks, self.charChunksLabels = self.makeCharChunks()\n",
    "        if self.chunks: \n",
    "            self.allText = self.charChunks\n",
    "            self.labels = self.charChunksLabels\n",
    "        self.lengths = [len(item) for item in self.allText]\n",
    "        return self.allText\n",
    "        \n",
    "    def makeCharDict(self): \n",
    "        \"\"\" Make a dictionary of each character's total speech. \"\"\"\n",
    "        # Initialize empty dictionary. \n",
    "        charDict = {char: \"\" for char in self.chars}\n",
    "        for elem in self.allSaidElems: \n",
    "            charDict[elem.attrib['who']]+=elem.text\n",
    "        return charDict\n",
    "            \n",
    "    def makeCharChunks(self, n=2): \n",
    "        \"\"\" Make a list of chunks of character speech. \"\"\"\n",
    "        charChunks = []\n",
    "        charChunksLabels = []\n",
    "        for char, text in self.charDict.items(): \n",
    "            #chunks = self.chunkText(text, n)\n",
    "            chunks = self.sliceText(text)\n",
    "            for i, chunk in enumerate(chunks): \n",
    "                charChunks.append(chunk)\n",
    "                charChunksLabels.append(char + '-%s' % i)\n",
    "        return charChunks, charChunksLabels\n",
    "        \n",
    "    def chunkText(self, text, n=2): \n",
    "        \"\"\" Breaks one text into n texts.\"\"\"\n",
    "        newLen = math.floor(len(text) / n)\n",
    "        parts = [text[i:i+newLen] for i in range(0, len(text), newLen)]\n",
    "        if len(parts) > n: \n",
    "            parts[-2]+=parts[-1]\n",
    "            parts = parts[:n]\n",
    "        return parts\n",
    "    \n",
    "    def sliceText(self, text, size=8000):\n",
    "        parts = []\n",
    "        while len(text) > size: \n",
    "            part = text[:size]\n",
    "            text = text[size:]\n",
    "            parts.append(part)\n",
    "        return parts\n",
    "\n",
    "    def clean(self, utterance): \n",
    "        \"\"\" \n",
    "        Cleans utterances. \n",
    "        \"\"\"\n",
    "        # Remove \"said Bernard,\" etc. \n",
    "        charRegex = \"said (%s),\" % '|'.join(self.chars)\n",
    "        out = re.sub(charRegex, '', utterance)\n",
    "       \n",
    "        # Remove quotation marks. \n",
    "        out = re.sub('[“”\"]', '', out)\n",
    "        \n",
    "        # Remove line breaks. \n",
    "        out = re.sub('\\n', ' ', out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code adapted from http://stackoverflow.com/a/28384887/584121\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs): \n",
    "        return\n",
    "\n",
    "    def get_params(self, deep=True): \n",
    "        \"\"\" Dummy method. \"\"\"\n",
    "        return {'None': 'None'}\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b9c5404e07d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnumLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericLabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "labels = text_clf.named_steps.get('text').labels\n",
    "numLabels = text_clf.named_steps.get('text').numericLabels\n",
    "word_names = text_clf.named_steps.get('tfidf').get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9f88062f5c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                          \u001b[0;34m(\u001b[0m\u001b[0;34m'bgm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBayesianGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         ])\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise ValueError(\n\u001b[0;32m--> 830\u001b[0;31m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \"string object received.\")\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "lenmin, lenmax = (0, 200000000)\n",
    "text = 'waves-tei.xml'\n",
    "text_clf = Pipeline([\n",
    "#                         ('text', Text(lenRange=(lenmin, lenmax))),\n",
    "                         ('tfidf', TfidfVectorizer(use_idf=False, max_features=100)),\n",
    "                         ('todense', DenseTransformer()),\n",
    "#                          ('SVD', TruncatedSVD()),\n",
    "                         ('PCA', PCA(n_components=10)),\n",
    "                         ('bgm', BayesianGaussianMixture(n_components=6)),\n",
    "                        ])\n",
    "tclf = text_clf.fit(text)\n",
    "predictions = tclf.predict(text)\n",
    "predictions    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = { 'tfidf__use_idf': (True, False), \n",
    "               'tfidf__max_features': (100, 200, 500), \n",
    "               'pca__n_components': (2, 5, 10, 20, 30, 50)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyo! fitting\n",
      "heyo! transforming\n"
     ]
    }
   ],
   "source": [
    "text = Text().fit()\n",
    "docs = text.transform('waves-tei.xml')\n",
    "labels = text.numericLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                         ('tfidf', TfidfVectorizer()),\n",
    "                         ('todense', DenseTransformer()),\n",
    "                         ('pca', PCA(n_components=10)),\n",
    "                         ('bgm', BayesianGaussianMixture(n_components=6)),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['tfidf', 'todense', 'pca', 'bgm']\n",
      "parameters:\n",
      "{'pca__n_components': (2, 5, 10, 20, 30, 50),\n",
      " 'tfidf__max_features': (100, 200, 500),\n",
      " 'tfidf__use_idf': (True, False)}\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 8.285s\n",
      "\n",
      "Best score: 14.794\n",
      "Best parameters set:\n",
      "\tpca__n_components: 20\n",
      "\ttfidf__max_features: 500\n",
      "\ttfidf__use_idf: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:    8.0s finished\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(docs, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svd = text_clf.named_steps.get('SVD')\n",
    "# x, y = svd.components_[0,:], svd.components_[1,:]\n",
    "pca = text_clf.named_steps.get('PCA')\n",
    "x, y = pca.components_[0], pca.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.adjusted_rand_score(numLabels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translateNumColor(colorList): \n",
    "    colorDict = 'rgbcymk'\n",
    "    return [colorDict[numColor] for numColor in colorList]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = translateNumColor(numLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.15, max_df=0.4)\n",
    "tfidfs = tfidf_vectorizer.fit_transform(docs).todense()\n",
    "df = pd.DataFrame(tfidfs, columns=tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addLabels(df): \n",
    "    df['label'] = t.labels\n",
    "    df['char'] = df['label'].str.split('-').str.get(0)\n",
    "    df['lengths'] = t.lengths\n",
    "    df['lengths'] = df['lengths'].divide(df['lengths'].max())\n",
    "    df['lengths'] = df['lengths'] * 40\n",
    "    charDict = {'Bernard': 0, 'Louis': 1, 'Neville': 2,\n",
    "                'Rhoda': 3, 'Jinny': 4, 'Susan': 5}\n",
    "    df['numLabel'] = df['label'].apply(lambda x: charDict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do latent semantic analysis dimensionality reduction. \n",
    "svd = TruncatedSVD(n_components=2)\n",
    "lsa_out = svd.fit_transform(df)\n",
    "lsaDF = pd.DataFrame(lsa_out, columns=['x','y'])\n",
    "addLabels(lsaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_out = pca.fit_transform(df)\n",
    "pcaDF = pd.DataFrame(pca_out, columns=['x', 'y'])\n",
    "addLabels(pcaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def translateColor(label): \n",
    "    colorDict = {'B': 'b', 'S': 'g', 'R': 'r', 'N': 'c', 'J': 'y', 'L': 'm'}\n",
    "    return colorDict[label[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotChars(df, dims=2): \n",
    "    if dims == 3: \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    for char in t.chars: \n",
    "        charDF = df.loc[df['char'] == char]\n",
    "        if dims == 2: \n",
    "            plt.scatter(charDF['x'], charDF['y'], color=translateColor(char), s=charDF['lengths'], label=char)\n",
    "            #plt.legend()\n",
    "        if dims == 3:         \n",
    "            ax.scatter(charDF['x'], charDF['y'], charDF['z'], color=translateColor(char), \n",
    "                       s=charDF['lengths'], label=char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doKMeans(df):\n",
    "    km = KMeans(n_clusters=6)\n",
    "    fitted = km.fit_transform(df[['x', 'y']])\n",
    "    means = np.argmax(fitted, axis=1)\n",
    "    df['mean']=means\n",
    "    colorMap = 'rgbcmyk'\n",
    "    for row in df.iterrows(): \n",
    "        plt.scatter(row[1]['x'], row[1]['y'], color=colorMap[row[1]['mean']])\n",
    "    print(metrics.adjusted_rand_score(pcaDF['numLabel'], pcaDF['mean']))\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def doInference(df): \n",
    "    gm = BayesianGaussianMixture(n_components=6, \\\n",
    "                                 covariance_type=\"full\")\n",
    "    dfXY = df[['x', 'y']]\n",
    "    fitted = gm.fit(dfXY)\n",
    "    colorMap = 'rgbcmyk'\n",
    "    df['mean'] = gm.predict(dfXY)\n",
    "    for row in df.iterrows():\n",
    "        plt.scatter(row[1]['x'], row[1]['y'], color=colorMap[row[1]['mean']])\n",
    "    print(metrics.adjusted_rand_score(df['numLabel'], df['mean']))\n",
    "    return df['mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = doKMeans(pcaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = doInference(lsaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotChars(pcaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotChars(lsaDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
